%% Put the following text in the intro somewhere
\begin{comment}
The algebra of observables in quantum theory plays a fundamental
role. When classical systems are quantized, their classical
symmetry algebra acting on a set of physical observables, in
simplest examples, remains the same. For some completely
integrable non-linear models, consistent quantization requires
that the classical symmetry group be replaced by a quantum group
\cite{frt,drinfeld,woronowicz,manin} via a deformation parameter
$q = 1 + O(\hbar)$. In recent years quantum groups involving
fermions have received widespread attention. These include
deformed fermion algebras \cite{jx,xh,sm,chung}, spin chains
\cite{nt,gppr,bnnpsw} and Fermi gases \cite{ubriaco}. At the same
time, some quantum systems, most notably fermionic quantum systems
do not have any classical analogues. Nevertheless, fermions are
perhaps the most important sector of quantum phenomena. Motivated
by these considerations, we define a fermionic version of the
angular momentum algebra by the relations
\end{comment}
%%


\section{Defining relations}

\bea
\anti{J_1}{J_2} & = & J_3 \label{eqn:defrel1} \\
\anti{J_2}{J_3} & = & J_1 \label{eqn:defrel2} \\
\anti{J_3}{J_1} & = & J_2 \label{eqn:defrel3}
\eea
where $J_1$, $J_2$, $J_3$ are hermitian generators of the algebra. We will name this algebra ACSA, the anticommutator spin algebra. In these expressions the curly bracket denotes the anticommutator \beq
\anti{A}{B} \equiv AB + BA \eeq so (\ref{eqn:defrel1}-\ref{eqn:defrel3}) should be taken as the
definition of an associative algebra. This proposed algebra does
not fall into the category of superalgebras in the sense of
Berezin-Kac axioms. In particular, the algebra is consistent
without grading and there are no (graded) Jacobi relations. As it
is defined this algebra falls into the category of a
(non-exceptional) Jordan algebra where the Jordan product is
defined by: \beq A \circ B \equiv \frac12 (AB + BA) \quad . \eeq A
formal Jordan algebra, in addition to a commutative Jordan
product, also satisfies $A^2\circ(B\circ A) = (A^2\circ B)\circ
A$. When the Jordan product is given in terms of an anticommutator
this relation is automatically satisfied. Just as a Lie algebra
where the Lie bracket as defined by the commutator leads to an
enveloping associative algebra, a Jordan algebra defined in terms
of the above product leads to an enveloping associative algebra
which we consider as an algebra of observables.

The physical properties of this system turn out to be similar to
those of the angular momentum algebra yet exhibit remarkable
differences. Since the angular momentum algebra is used to
describe various internal symmetries, ACSA could be relevant in
describing those symmetries.

In section 2 we will show that ACSA is invariant under the action
of the quantum group $SO_q(3)$ with $q=-1$. Here, $SO_q(3)$ is
defined as the quantum subgroup of $SU_q(3)$ where each of the
(non-commuting) matrix elements of the $3$x$3$ matrix is
hermitian. We note that this defines a quantum group only for
$q=\pm1$. For $q=1$ one has the real orthogonal group $SO(3)$.

In section 3, we will construct all representations of ACSA and
show that the representations can be labelled by a quantum number
$j$ corresponding to the eigenvalue of $J_3$ whose absolute value
is maximum. For integer $j$, spectrum of $J_3$ is given by $j,
j-1, \ldots, -j$ whereas for half-integer $j$ there are two
representations. These two representations are such that for $j =
2k\pm\frac12$ spectrum of $J_3$ is respectively given by $j, j-2,
\ldots, \pm\frac12$ and $-j, j+2, \ldots, \mp\frac12$. Section 4
is reserved for conclusions and discussion.

\section{The invariance quantum group $SO_{q = -1}(3)$}

In order to find the invariance quantum group of this algebra, we
transform the generators $J_i$ to $J'_i$ by:
\beq \label{trans}
J'_i = \sum_j\alpha_{ij} J_j \quad .
\eeq
The matrix elements
$\alpha_{ij}$ are hermitian since $J_i$'s are hermitian and they
commute with $J_i$'s but are not assumed to commute with each other. For the
transformed operators to obey the original relations, there should
exist some conditions on the $\alpha$'s which define the
invariance quantum group of the algebra. It is very convenient at
this moment to switch to an index notation that encompasses all
three defining relations of the algebra in one index equation. For
the angular momentum algebra this is possible by defining the
totally anti-symmetric rank 3 pseudo-tensor $\epsilon_{ijk}$. A
similar object for ACSA which we will call the fermionic
Levi-Civita tensor, $u_{ijk}$, is defined as:
\beq
u_{ijk} =
  \begin{cases}
    1, & \text{for $i \neq j \neq k \neq i$,} \\
    0, & \text{otherwise.}
  \end{cases}
\eeq
Thus the defining relations (\ref{eqn:defrel1}-\ref{eqn:defrel3}) become:
\beq
\anti{J_i}{J_j} = \sum_k  u_{ijk}\,J_k + 2\delta_{ij}\,J_i^2
\eeq
The second term on the right is needed since when $i = j$ the left-hand side becomes $2J_i^2$.
Upon transformation (\ref{trans}) we require the algebra relations to remain invariant which means:
\beq
\anti{J'_i}{J'_j} = J'_k\quad\quad\text{for $i \neq j \neq k \neq i$}.
\eeq
However, substituting the transformation equations into the
left-hand side, we have:
\beq
\anti{J'_i}{J'_j} =
\sum_{k,\;m} \left(\alpha_{ik} \alpha_{jm} J_k J_m + \alpha_{jm} \alpha_{ik} J_m J_k \right)
\eeq
If one considers the quadratic forms in the universal enveloping algebra of ACSA, then
one can see that the symmetric part of these forms resolve to linear forms owing to
the defining relations of the algebra. Thus the independent quadratic forms in the
algebra are the antisymmetric forms, $[J_m, J_k]$ where $m \neq k$, and the square forms,
$J^2_k$. Using this observation we put the above relation in the form of a
linear sum over independent algebra elements:
\beq
\begin{split}
\anti{J'_i}{J'_j}
& = \sum_{n,\;m} \left(\alpha_{in} \alpha_{jm} J_n J_m + \alpha_{jm} \alpha_{in} J_m J_n \right)\\
& = \frac12 \sum_{n,\;m} \left(\alpha_{in} \alpha_{jm} (\anti{J_n}{J_m} + [J_n, J_m])
  + \alpha_{jm} \alpha_{in} (\anti{J_m}{J_n} + [J_m, J_n]) \right) \\
& = \frac12 \sum_{\substack{n,\;m \\n \neq m}} \left(\sum_{l}(\alpha_{in} \alpha_{jm} + \alpha_{jm} \alpha_{in}) u_{nml} J_l
    + (\alpha_{in} \alpha_{jm} - \alpha_{jm} \alpha_{in}) [J_n, J_m]\right) \\
& \quad + \sum_{n} \left(\alpha_{in} \alpha_{jn} + \alpha_{jn} \alpha_{in}\right) J^2_n
\end{split}
\eeq
which should be equal to $J'_k$ for $i \neq j \neq k \neq i$, which in turn gives:
\begin{gather}
\anti{J'_i}{J'_j} = J'_k    \\
\begin{split}
&\sum_{n} \left(\alpha_{in} \alpha_{jn} + \alpha_{jn} \alpha_{in}\right) J^2_n \\
& \quad + \frac12 \sum_{\substack{n,\;m \\n \neq m}} \left(\sum_{l}(\alpha_{in} \alpha_{jm} + \alpha_{jm} \alpha_{in}) u_{nml} J_l
    + (\alpha_{in} \alpha_{jm} - \alpha_{jm} \alpha_{in}) [J_n, J_m]\right) \\
& \qquad \qquad = \sum_{l} \alpha_{kl} J_l \qquad \text{for $i \neq j \neq k \neq i$.}\\
\end{split}
\end{gather}
This final equation yields the following relations among $\alpha_{ij}\;$:
\begin{align}
\alpha_{in} \alpha_{jn} + \alpha_{jn} \alpha_{in} & = 0  && \text{for $i \neq j$} \label{invrel1'} \\
\alpha_{in} \alpha_{jm} - \alpha_{jm} \alpha_{in}& = 0 && \text{for $i \neq j$ and $n \neq m$} \label{invrel2'} \\
\frac12 \sum_{\substack{n,\;m \\n \neq m}}(\alpha_{in} \alpha_{jm} + \alpha_{jm} \alpha_{in}) u_{nml} & = \alpha_{kl} && \text{for $i \neq j \neq k \neq i$} \label{invrel3'}
\end{align}
However, by virtue of \eqref{invrel2'} and the fact that $u_{ijk} = 0$ if
any two indices are the same, the relation \eqref{invrel3'} can be written as:
\beq
\begin{split}
\alpha_{kl} & = \frac12 \sum_{n,\;m}(\alpha_{in} \alpha_{jm} + \alpha_{jm} \alpha_{in}) u_{nml} \\
& = \frac12 \sum_{n,\;m}2 \alpha_{in} \alpha_{jm} u_{nml} \\
& = \sum_{n,\;m}\alpha_{in} \alpha_{jm} u_{nml} \quad \text{for $i \neq j \neq k \neq i$}
\end{split}
\eeq
Therefore the resulting relations between the $\alpha_{ij}$ that define the invariance
group of this algebra becomes:
\begin{align}
\alpha_{in} \alpha_{jn} + \alpha_{jn} \alpha_{in} & = 0  && \text{for $i \neq j$} \label{invrel1} \\
\alpha_{in} \alpha_{jm} - \alpha_{jm} \alpha_{in}& = 0 && \text{for $i \neq j$ and $n \neq m$} \label{invrel2} \\
\sum_{n,\;m} \alpha_{in} \alpha_{jm} u_{nml} & = \alpha_{kl} && \text{for $i \neq j \neq k \neq i$} \label{invrel3}
\end{align}

Before we define the quantum group $SO_q(3)$ and show that the
relations above correspond to the case $q=-1$, we first define the quantum
general linear group $GL_{q}(2)$. This group is defined by the elements:
\beq
M =
  \begin{pmatrix}
    a & b \\
    c & d
  \end{pmatrix}
\eeq
such that the matrix elements satisfy the following relations:
\bea
a b & = & q b a \label{ab} \\
b d & = & q d b \label{bd} \\
a c & = & q c a \label{ac} \\
c d & = & q d c \label{cd} \\
a d - q b c & = & d a - q^{-1} c b \label{det} \\
b c & = & c b \label{bc}
\eea
Using this definition of $GL_q(2)$, we first define $GL_q(3)$ as the set of
matrices:
\beq
A =
  \begin{pmatrix}
    A_{11} & A_{12} & A_{13} \\
    A_{21} & A_{22} & A_{23} \\
    A_{31} & A_{32} & A_{33}
  \end{pmatrix} \in GL_q(3)
\eeq
where
\beq
\label{gl2}
\begin{pmatrix}
  A_{in} & A_{im} \\
  A_{jn} & A_{jm}
\end{pmatrix}
\in GL_{q}(2) \quad \text{for $i \neq j$ and $m \neq n$}\quad .
\eeq
From $GL_q(3)$, one can obtain the quantum special linear group in
3-dimensions, $SL_q(3)$ by imposing the condition:
\beq
det_{q}(A) = 1
\eeq
where the quantum determinant is defined as presented in the Introduction.
Furthermore, on $SL_q(3)$, one can impose the "reality" condition:
\beq
A_{ij} = A^*_{ij}
\eeq
thus ending up the quantum group $SL_q(3, \IR)$. On the other hand, one
can impose the unitarity condition:
\beq
A^\dagger = A^{-1}
\eeq
on $SL_q(3)$ and
obtain the quantum group $SU_q(3)$. The quantum group $SO_q(3)$
is equivalent to the quantum group
\mbox{$SL_q(3, \IR) \cap SU_q(3)$}. However one can show for
$SL_q(3, \IR)$ that $q= e^{i\beta}$ for some $\beta \in \IR$ and
similarly for $SU_q(3)$ that $q \in \IR$. Thus one finds that $q =
\pm 1$ for $SO_q(3)$. When $q = 1$ the quantum group becomes the
usual $SO(3)$ group; the interesting case is when $q = -1$ which,
as we will show, is the invariance quantum group of ACSA.

By virtue of the relation \eqref{gl2} and the relations \eqref{ac} - \eqref{bc}
between the matrix elements of a $GL_q(2)$ matrix, we can see that for
the case when $q = -1$, we have the following relations between the matrix
elements of $SO_q(3)$:
\bea
A_{in} A_{jn} & = & - A_{jn} A_{in} \label{gl2-res1} \\
A_{in} A_{jm} + A_{im} A_{jn} & = & A_{jm} A_{in} + A_{jn} A_{im} \label{gl2-res2} \\
A_{im} A_{jn} & = & A_{jn} A_{im} \label{gl2-res3}
\eea
for $i \neq j$ and $n \neq m$. Furthermore, using \eqref{gl2-res3} in \eqref{gl2-res2} one finds:
\beq \label{gl2-res4}
A_{in} A_{jm} = A_{jm} A_{in}
\eeq
again for $i \neq j$ and $n \neq m$.

Thus for a matrix $A \in SO_{q=-1}(3)$, the transformation invariance relation
\eqref{invrel1} is shown to be satisfied by virtue of relation \eqref{gl2-res1}.
Similarly, the elements of such a matrix satisfy the relation \eqref{invrel2}
by virtue of the $GL_q(2)$ relations \eqref{gl2-res3} and \eqref{gl2-res4}.

It is a little harder to show that equation \eqref{invrel3} is
satisfied by elements of $SO_{q=-1}(3)$ matrices. However, if one considers
a particular choice of $k$ and $l$ on the right hand side of this equation, one
can see that on the left hand side one has the freedom to choose $i$ and $j$ in two
different ways. This implies that a particular $\alpha_{kl}$ is
equal to two separate forms. Given explicitly, for a given choice of $k$ and
$l$, we get:
\bea
\alpha_{kl} &=& \sum_{r,\; q} \alpha_{ir} \alpha_{jq} u_{rql} \\
\alpha_{kl} &=& \sum_{r,\; q} \alpha_{jr} \alpha_{iq} u_{rql}
\eea
for a particular choice of $i$ and $j$ such that $i, j, k$ are all different.
In each of these sums only two terms survive, one where $r = n$, $q = m$
and the other one where $r = m$, $q = n$, such that $n, m, l$ all different. This
is due to the nature of $u_{ijk}$ which is non-zero only if all the indices are
different. Finally we arrive at the explicit form of relation \eqref{invrel3}:
\beq
\alpha_{kl} = \alpha_{in} \alpha_{jm} + \alpha_{im} \alpha_{jn} = \alpha_{jm} \alpha_{in} +  \alpha_{jn} \alpha_{im}
\eeq
for $i, j, k$ all different and $n, m, l$ all different. Written in this form, it is obvious
that due to the $GL_q(2)$ relation \eqref{gl2-res2}, part of the above equality is satisfied by the
matrix elements of a matrix in $SO_{q=-1}(3)$. The fact that both sides of this relation
is equal to another matrix element does not rise form the $GL_q(2)$ relations but is
due to the fact that the matrix is special and orthogonal, i.e. it is due to the fact that
$A^T = A^{-1}$ and that $\det_{q=-1}A = 1$. In order to show this, we should first note that
$\det_q$ where $q = -1$ is the same as the normal determinant except there is no alternation
of signs as there is in the normal determinant; this type of
determinant with no alternation of signs is also called a
permanent. Given this fact, one can notice that the $GL_{q=-1}(2)$
relation \eqref{gl2-res2} is equal to the determinant of the $GL_{q=-1}(2)$ submatrix and
is nothing but the statement that this determinant is defined and unique.
The inverse of a matrix, $A^{-1}$, is defined as:
\beq \label{matinverse}
A^{-1} = \frac{1}{\det A} adj(A)^T
\eeq
where $adj(A)$ stands for the adjoint matrix where each matrix element is equal to the
cofactor of the same position element in the original matrix A.
For a matrix $A \in SO_{q=-1}(3)$; however, we have the fact that $A^{-1} = A^T$ and
$\det_{q = -1} A = 1$, thus for such a matrix, the relation \eqref{matinverse} becomes:
\beq
A^T = adj(A)^T \qquad \Rightarrow \qquad A = adj(A)
\eeq
which implies that the matrix elements of $A$ and $adj(A)$ are equal. This further implies
that each matrix element of $A$ is equal to the cofactor of itself. For $SO_{q=-1}(3)$
matrices, the cofactor of a matrix element is equal to the $q = -1$ determinant of its
$GL_{q=-1}(2)$ minor submatrix.
Thus, as a result of this argument, we have:
\beq
A_{kl} = cof_{kl}(A) = A_{in} A_{jm} + A_{im} A_{jn} = A_{jm} A_{in} +  A_{jn} A_{im} \quad ,
\eeq
thereby, showing that matrices which are elements of $SO_{q=-1}(3)$ fully satisfy the
transformation relations that leave ACSA invariant.

Thus, we have found that the invariance quantum group of ACSA is
the quantum group $SO_q(3)$ with $q=-1$. Strictly speaking, ACSA
is a module of the $q$-deformed $SO(3)$ quantum algebra with $q =
-1$. It is very interesting to note that the invariance group of
the angular momentum algebra is also $SO_q(3)$ but with $q=1$.

\section{Representations}

The Anticommutator Spin Algebra is defined by the relations
(\ref{eqn:defrel1}-\ref{eqn:defrel3}). In order to find the
representations of this algebra we define the operators:
\bea
J_+ & = & J_1 + J_2 \\
J_- & = & J_1 - J_2 \\
J^2 & = & J_1^2 + J_2^2 + J_3^2
\eea
which obey the following relations:
\bea
\anti{J_+}{J_3} & = & J_3 \\
\anti{J_-}{J_3} & = & -J_3 \\
J_+^2 & = & J^2 - J_3^2 + J_3 \label{jp^2}\\
J_-^2 & = & J^2 - J_3^2 - J_3 \label{jm^2}
\eea
Furthermore, it can easily be shown that $J^2$ is central in the algebra, i.e. that it commutes with all the elements of the algebra, by first observing that:
\bea
J^2_j \; J_i &=& J_j (J_k - J_i \; J_j) \nonumber \\
          &=& J_j J_k - (J_k - J_i \; J_j) J_j \nonumber \\
          &=& (J_j \; J_k - J_k \; J_j) + J_i \; J^2_j \nonumber \\
          &=& (2J_j \; J_k - J_i) + J_i \; J^2_j \quad \mbox{for}\quad  i \neq j \neq k \neq i.
\eea
Using this relation and the fact that $J^2 = \sum_j J_j$, we can see:
\bea
J^2 \; J_i &=& \sum_j J^2_j \; J_i \nonumber \\
        &=& J^3_i + \sum_{j \neq j} J^2_j \; J_i \nonumber \\
        &=& J^3_i + \sum_{j \neq i} (2J_j \; J_k - J_i + J_i \; J^2_j)
\eea
However, in the final form of this expression the sum only contains
two terms where the two indices $j$ and $k$ are symmetric. Thus the
whole expression can be written as:
\bea
J^2 \; J_i &=& J^3_i + \sum_{j \neq i} (2J_j \; J_k - J_i + J_i \; J^2_j) \nonumber \\
        &=& J^3_i - 2J_i + 2(J_j \; J_k + J_k \; J_j) + J_i \; J^2_j + J_i \; J^2_k \nonumber \\
        &=& J_i \; J^2 - 2J_i + 2J_i \nonumber \\
        &=& J_i \; J^2 \quad \mbox{for}\quad  i \neq j \neq k \neq i,
\eea
and therefore showing that $J^2$ commutes with all the elements of the algebra.

For this reason, we can label the states in our representation with the eigenvalues of $J^2$ and $J_3$:
\bea
J^2 \ket{\lambda, \mu} & = & \lambda \ket{\lambda, \mu} \\
J_3 \ket{\lambda, \mu} & = & \mu \ket{\lambda, \mu}
\eea
The action of $J_+$ and $J_-$ on the states such defined is easily
shown to be:
\bea
J_+ \ket{\lambda, \mu} & = & f(\lambda, \mu) \ket{\lambda,- \mu + 1} \label{jplus}\\
J_- \ket{\lambda, \mu} & = & g(\lambda, \mu) \ket{\lambda,- \mu -
1} \label{jminus}
\eea
It is enough to look at the norm of the states $J_+ \ket{\lambda, \mu}$ and $J_- \ket{\lambda, \mu}$ to find $f(\lambda, \mu)$ and $g(\lambda, \mu)$. Thus:
\bea
\bra{\lambda, \mu} J_+^2 \ket{\lambda, \mu} & = & |f(\lambda, \mu)|^2 \\
\bra{\lambda, \mu} J^2 - J_3^2 + J_3 \ket{\lambda, \mu} & = & |f(\lambda, \mu)|^2 \\
\lambda - \mu^2 + \mu & = & |f(\lambda, \mu)|^2 \\
f(\lambda, \mu) & = & \sqrt{\lambda - \mu^2 + \mu}
\eea
and, similarly, $g(\lambda, \mu) = \sqrt{\lambda - \mu^2 - \mu}$. These coefficients must be real due to the fact that $J_+$ and $J_-$ are hermitian operators. This constraint imposes the following
conditions on $\lambda$ and $\mu$:
\bea
\lambda - \mu^2 + \mu & \geq & 0 \\
\lambda - \mu^2 - \mu & \geq & 0
\eea
which can be satisfied by letting $\lambda = j(j+1)$ for some $j$ with:
\beq
j \geq \mu \geq -j. \label{muspec}
\eeq

Note that equation (\ref{jplus}) shows that the action of $J_+$ is
composed of a reflection which changes sign of $\mu$, the
eigenvalue of $J_3$, followed by raising by one unit. Similarly,
equation (\ref{jminus}) shows that $J_-$ reflects and lowers. Thus
the highest state $\mu = j$ is annihilated by $J_-$ and "lowered"
by $J_+$. Applying $J_+$ or $J_-$ twice to any state gives back
the same state due to relations (\ref{jp^2}) and (\ref{jm^2}).
Thus starting from the highest state we apply $J_-$ and $J_+$
alternately to get the spectrum: \beq j, -j+1, j-2, -j+3, ... \eeq
This sequence ends so as to satisfy equation (\ref{muspec}) only
for integer or half-integer $j$. For integer $j$, it terminates,
after an even number of steps, at $-j$ and visits every integer in
between only once. For half-integer $j=2k\pm\frac{1}{2}$ it ends
at $j=\pm\frac{1}{2}$ having visited only half the states with
$\mu$ half-odd integer between $j$ and $-j$. The rest of the
states cannot be reached from these but are obtained by starting
from the $\mu= -j$ state and applying $J_-$ and $J_+$ alternately;
starting with $J_-$.
\\
We now give a few examples:

\begin{figure}[!h]
  \[
  \xymatrix@R=20pt@C=100pt{
    \ar@(ul,dl)[]_*+{J_+} \ar@{-}[r]^-*+{0} & \ar@(ur,dr)[]^*+{J_-}
  }
  \]
  \caption{State diagram for $j=0$}
  \label{J0Diagram}
\end{figure}
\begin{figure}[!h]
  \[
  \xymatrix@R=20pt@C=100pt{
    \ar@/_/[d]_*+{J_+} \ar@{-}[r]^-*+{+1} & \\
    \ar@/_/[d]_*+{J_-} \ar@{-}[r]^-*+{0} & \\
    \ar@{-}[r]^-*+{-1} &  \\
  }
  \]
  \caption{State diagram for $j=1$}
  \label{J1Diagram}
\end{figure}
\begin{figure}[!h]
  \[
  \xymatrix@R=20pt@C=100pt{
    \ar@(dl,ul)[ddd]_<<<<<*+{J_+} \ar@{-}[r]^-*+{+2} & \\
    \ar@(dl,ul)[ddd]_>>>>>*+{J_-} \ar@{-}[r]^-*+{+1} & \\
    \ar@{-}[r]^-*+{0} & \ar@/_/[u]_-*+{J_+} \\
    \ar@{-}[r]^-*+{-1} & \ar@/_/[u]_-*+{J_-} \\
    \ar@{-}[r]^-*+{-2} &
  }
  \]
  \caption{State diagram for $j=2$}
  \label{J2Diagram}
\end{figure}
\begin{figure}[!h]
  \[
  \xymatrix@R=20pt@C=50pt{
    \ar@(ul,dl)[]_*+{J_+} \ar@{-}[rr]^-*+{\frac12} & & \\
                                                  & \ar@{-}[rr]^-*+{-\frac12} & & \ar@(ur,dr)[]^*+{J_-} \\
  }
  \]
  \caption{State diagram for $j=\frac12$}
  \label{J1/2Diagram}
\end{figure}
\begin{figure}[!h]
  \[
  \xymatrix@R=20pt@C=50pt{
    \ar@/_/[dd]_*+{J_+} \ar@{-}[rr]^-*+{\frac32} &  & \\
                                                 & \ar@{-}[rr]^-*+{\frac12} &   & \\
    \ar@{-}[rr]^-*+{-\frac12} & *\txt<2pc>{}      &  & \\
                                                 & \ar@{-}[rr]^-*+{-\frac32} &   & \ar@/_/[uu]^*+{J_-}\\
  }
  \]
  \caption{State diagram for $j=\frac32$}
  \label{J3/2Diagram}
\end{figure}
\pagebreak
\begin{comment}
\begin{itemize}
  \item
For $\mathbf{j=2}$ the states follow the sequence:
\[
\mathbf{\mu = 2, -1, 0, 1, -2}\quad.
\]
  \item
For $\mathbf{j=\frac{3}{2}}$ there exist two irreducible
representations one with:
\[
\mathbf{\mu = \frac{3}{2}, -\frac{1}{2}}\quad,
\] and the other with:
\[
\mathbf{\mu = -\frac{3}{2}, \frac{1}{2}}\quad.
\]

  \item
For $\mathbf{j=\frac{5}{2}}$ the two representations are given by:
\[
\mathbf{\mu = \frac{5}{2}, -\frac{3}{2}, \frac{1}{2}}\quad,
\] and by:
\[
\mathbf{\mu = -\frac{5}{2}, \frac{3}{2}, -\frac{1}{2}}\quad.
\]

\end{itemize}
\end{comment}

\section{Hopf Algebra Structure with braiding}

One natural question to ask having considered this associative algebra is whether or not it has a Hopf algebra structure. On the surface, this algebra shares a lot with its sister algebra, the $SU(2)$ Lie algebra, which has a Hopf algebra structure and one would expect ACSA to similarly have one. It turns out, however, that naively trying the same coproduct rule for ACSA does not work due to the symmetric nature of the product defined on ACSA since the product is defined in terms of anticommutators. As was noted in the Introduction of this work, the coproduct of the Lie algebra requires the product on the Lie algebra to be anti-symmetric. For this reason, the coproduct of the $SU(2)$ Lie algebra is not suitable for ACSA.

In our quest for a Hopf algebra structure for ACSA, it would be more fruitful to understand the nature of the relationship of ACSA with the $SU(2)$ Lie algebra. If one names the generators of the $SU(2)$ algebra $I_i$, then it can easily be shown that $\tilde{J_i}$ defined as
\beq
\tilde{J}_i = - I_i \otimes \sigma_i
\eeq
satisfy the defining relation of ACSA since:
\bean
\tilde{J}_i \tilde{J}_j + \tilde{J}_j \tilde{J}_i &=& I_i I_j \otimes \sigma_i \sigma_j + I_j I_i \otimes \sigma_j \sigma_i\\
&=& I_i I_j \otimes i \sigma_k + I_j I_i \otimes -i \sigma_k \\
&=& i(I_i I_j - I_j I_i) \otimes \sigma_k \\
&=& i(iI_k) \otimes \sigma_k \\
&=& - I_k \otimes \sigma_k \\
&=& \tilde{J}_k
\quad \mbox{for}\quad  i \neq j \neq k \neq i.
\eean

Similarly, the generators satisfying the $SU(2)$ Lie algebra can be written in terms of ACSA generators as:
\beq
\tilde{I}_i = J_i \otimes \sigma_i
\eeq
since:
\bean
\tilde{I}_i \tilde{I}_j - \tilde{I}_j \tilde{I}_i &=& J_i J_j \otimes \sigma_i \sigma_j - J_j J_i \otimes \sigma_j \sigma_i\\
&=& J_i J_j \otimes i \sigma_k - J_j J_i \otimes -i \sigma_k \\
&=& i(J_i J_j + J_j J_i) \otimes \sigma_k \\
&=& i(J_k) \otimes \sigma_k \\
&=& \tilde{I}_k
\quad \mbox{for}\quad  i \neq j \neq k \neq i.
\eean

These two relations show that the $SU(2)$ algebra and ACSA are so closely related that it is not even possible to identify which one of these algebras is more fundamental. Both of them can be written in terms of the generators of the other and their algebraic structure can be derived from the structure of the other one. However, as mentioned, the Hopf algebra structure of ACSA cannot be derived from the Hopf algebra structure of the $SU(2)$ algebra. Specifically, ACSA does not admit a coproduct defined in a normal way using the usual tensor products. Such a coproduct can be defined if one were to extend the permutation operator $\tau$ used in the connecting relation. Normally the operation of $\tau$ is defined as:
\beq
\tau(A \otimes B) = B \otimes A \quad ,
\eeq
however, if one considers the algebra to be graded and one were to define a degree operator ($deg$) which is $0$ for bosonic variables and is $1$ for fermionic variables, then the natural redefinition of the $\tau$ operator is
\beq
\tau(A \otimes B) = (-1)^{deg\;A\; deg\;B\;} B \otimes A \quad .
\eeq
Using this redefined permutation operator, one can still write down the Hopf algebra relations and only the connecting relation will be the one that will be redefined; thus, we arrive at a braided Hopf algebra structure.

When the permutation operator is redefined in this way, the product of two tensor product terms is given by $(A\otimes B) (C\otimes D) = (-1)^{deg\;B\; deg\;C\;} (AC \otimes BD)$ where the $-1$ factor comes in because of the reordering of the $B$ and $C$ terms. Using this rule and defining the degree of $1$ as $0$ and the degrees of $J_1, J_2, J_3$ as $1$, we can see that the coproduct defined as:
\bea
\Delta(J_i) &=& 1 \otimes J_i + J_i \otimes 1 \\
\Delta(1) &=& 1 \otimes 1
\eea
satisfies the algebra structure relations since:
\bean
\Delta(J_i)\Delta(J_j)
&=& (1 \otimes J_i + J_i \otimes 1)(1 \otimes J_j + J_j \otimes 1) \\
&=& 1 \otimes J_i J_j - 1 J_j \otimes J_i 1 + J_i 1 \otimes 1 J_j + J_i J_j \otimes 1 \\
&=& 1 \otimes J_i J_j - J_j \otimes J_i + J_i \otimes J_j + J_i J_j \otimes 1
\eean
and
\bean
\Delta(J_i)\Delta(J_j) + \Delta(J_j)\Delta(J_i)
&=& 1 \otimes J_i J_j - J_j \otimes J_i + J_i \otimes J_j + J_i J_j \otimes 1 \\
& & + 1 \otimes J_j J_i - J_i \otimes J_j + J_j \otimes J_i + J_j J_i \otimes 1 \\
&=& 1 \otimes J_i J_j + J_i J_j \otimes 1 \\
&=& 1 \otimes J_k + J_k \otimes 1 \\
&=& \Delta(J_k)
\quad \mbox{for}\quad  i \neq j \neq k \neq i.
\eean
The counit and coinverse are simpler and they match with the definitions for the normal Lie algebra, i.e.:
\bea
\epsilon(J_i) &=& 0 \\
S(J_i) &=& -J_i
\eea
These definitions of the coproduct, the counit and the coinverse give us a braided Hopf algebra structure for ACSA. 